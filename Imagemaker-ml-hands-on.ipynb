{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/welcome.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ##  Antes de partir\n",
    "1. Se utilizaron los laboratorios creados por Microsoft como referencia, ellos se encuentran en https://github.com/MicrosoftDocs/mslearn-aml-labs. Los datos fueron obtenidos de ese repositorio también.\n",
    "2. Verifica que creaste correctamente el recurso y la instancia.\n",
    "3. Es importante clonar el repositorio https://github.com/MauricioLetelier/hands-on-MLOps, para esto abre el terminal, ingresa a la carpeta Users y finalmente clona el repositorio.\n",
    "4. Verifica que abriste este jupyter notebook en la opción \"Python 3.6 - AzureML\".\n",
    "\n",
    "## Verifiquemos la versión\n",
    "Nunca está de más verificar que tengamos la misma versión de Azureml, en caso contrario podrían haber ciertos problemas de compatibilidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "print(\"¿Listos para ensuciarse las manos con la versión:\", azureml.core.VERSION, \"de Azureml?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición del Workspace\n",
    "Algo fundamental para poder trabajar correctamente es la definición del Worskpace. Por lo general, se puede guardar un archivo JSON en el cual están los datos asociados al Workspace, lo que permite llamar al archivo sin necesidad de poner en el código las credenciales. En este caso, dado que estamos utilizando una instancia, se puede invocar el método from_config() sin necesidad de haber generado el archivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(\"el workspace\",ws.name, \"se cargó correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Con qué datos trabajaremos?\n",
    "La idea es poder trabajar con un dataset que ya tenga cierto trabajo de preprocesamiento, por lo que una buena alternativa es usar el dataset de \"diabetes\" que tenemos en la carpeta de nuestro repositorio. En este, se tienen varios indicadores del paciente, y a la vez se tiene en una columna si el paciente fue diagnosticado con diabetes o no."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datastores\n",
    "Los datastores, son \"storages\" que se asocian a nuestro workspace. Por default al momento de crear el recurso de machine learning, se crean un blobstore y un filestore, pero también podemos asociar a nuestro ambiente algún otro recurso en el que tengamos nuestro datos. Los más populares son: Azure Blob Container, Azure File Share, Azure Data Lake, Azure Data Lake Gen2, Azure SQL Database, Azure Database for PostgreSQL, Databricks File System y Azure Database for MySQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the default datastore\n",
    "nuestro_datastore = ws.get_default_datastore()\n",
    "\n",
    "# Enumerate all datastores, indicating which is the default\n",
    "for nombres_datastores in ws.datastores:\n",
    "    print(nombres_datastores+',', \"¿está marcado como default? =>\", nombres_datastores == nuestro_datastore.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para subir nuestros datos a los datastores establecidos, utilizamos upload_files, el cual nos permite buscar un archivo en la ruta entregada para finalmente guardar los datos en un target path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuestro_datastore.upload_files(files=['./data/diabetes.csv'],\n",
    "                       target_path='diabetes-azure1/', \n",
    "                       overwrite=True, # Replace existing files of the same name\n",
    "                       show_progress=True)\n",
    "\n",
    "nuestro_datastore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "Para poder referenciar y versionar de mejor manera los datos que guardamos en nuestros datastores, es necesario una abstracción diferente que nos permita conseguir esta funcionalidad. Por ello es que existen los datasets, ellos pueden ser tabulares o \"file-based\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File-based Datasets\n",
    "Muchas veces los datos que necesitamos utilizar no son estructrados. En estos casos la mejor opción es utilizar este tipo de abstracción. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Dataset\n",
    "\n",
    "nuestro_datastore = ws.get_default_datastore()\n",
    "\n",
    "file_data_set = Dataset.File.from_files(path=(nuestro_datastore, 'diabetes-azure1/diabetes.csv'))\n",
    "file_data_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vemos, ya se encuentra el file_data_set en nuestro workspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabular Datasets\n",
    "En otras oportunidades, podremos acceder a nuestros datos en un formato tabular. Este es el ejemplo clásico de datos estructurados, y los datasets tienen funcionalidades específicas que nos ayudaran a trabajar de manera más sencilla con los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_data_set = Dataset.Tabular.from_delimited_files(path=(nuestro_datastore, 'diabetes-azure1/diabetes.csv'))\n",
    "\n",
    "tab_data_set.to_pandas_dataframe().info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¡Registrando nuestros datasets!\n",
    "Como mencionabamos con anterioridad, una de las razones más importantes para trabajar con datasets es el concepto del versionamiento de datos. En este caso, para poder generar nuestras versiones, debemos registrar cada uno de nuestros datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_data_set = file_data_set.register(workspace = ws,\n",
    "                                  name='diabetes-azure',\n",
    "                                  description='diabetes file dataset',\n",
    "                                  create_new_version=True)\n",
    "file_data_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a revisar en el módulo de Azure de machine learning para verificar que efectivamente se haya registrado.\n",
    "\n",
    "## ¿Cómo vamos a buscar los datos?\n",
    "La forma de ir a buscar los se puede hacer de diferentes maneras, una de ellas es ir a buscarlas por nombre. Además podemos pasar nuestros a un clásico dataframe de pandas o de spark para cargas más intensivas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from azureml.core import Dataset\n",
    "\n",
    "dataset = Dataset.get_by_name(workspace = ws, name='diabetes-azure')\n",
    "\n",
    "dataset.to_pandas_dataframe().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Qué pasó?\n",
    "Al momento de cargar nuestros datos utilizamos una carga como file_data_set, lo que carga los datos como archivo, pero para cargarlo directamente con to_pandas_dataframe, debimos utilizar un registro tabular.\n",
    "Esto es importante puesto que nos permite identificar una de las fortalezas de cargar datasets tabulares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_data_set = tab_data_set.register(workspace = ws,\n",
    "                                  name='diabetes-azure',\n",
    "                                  description='diabetes tabular dataset',\n",
    "                                  create_new_version=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.get_by_name(workspace = ws, name='diabetes-azure')\n",
    "\n",
    "dataset.to_pandas_dataframe().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¡Perfecto! Ya cargamos nuestro dataset y podemos trabajar con los datos asociados!\n",
    "## ¿Puedo revisar otras versiones?\n",
    "\n",
    "Si explicitamos la versión podemos ir a buscar el dataset que hayamos cargado en ese momento, a continuación podemos probar ambas versiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset.get_by_name(workspace = ws, name='diabetes-azure').to_pandas_dataframe().head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset.get_by_name(workspace = ws, name='diabetes-azure',version=1).to_pandas_dataframe().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¡Break!\n",
    "Si se te están fundiendo las neuronas, lo mejor es que paremos unos minutos de manera que podamos conversar y relajarnos antes de entrar a la siguiente parte del workshop!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Volvamos a enfocarnos = ¡Experimentos!\n",
    "En un comienzo podemos revisar la mecánica de los experimentos. Como vemos al momento de iniciar un experimento debemos asociarlo al workspace, para luego asignarle un nombre que será su identificador.\n",
    "## Guardando información con logs\n",
    "Luego, dado un experimento iniciado, podemos guardar distintos tipos de métricas asociadas a logs. Lo genial que tiene es que nos permite guardar números, listas, tablas e incluso gráficos entre otros. Como veremos a continuación iniciamos el tracking una vez se inicializa el start_logging(). Para asociar logs, basta asociarlos al experimento.start_logging()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "\n",
    "experimento = Experiment(workspace = ws, name = \"ml-imagemaker-1\")\n",
    "\n",
    "run = experimento.start_logging()\n",
    "RunDetails(run).show()\n",
    "\n",
    "print(\"Iniciando el experimento:\", experimento.name)\n",
    "\n",
    "df = Dataset.get_by_name(workspace = ws, name='diabetes-azure').to_pandas_dataframe()\n",
    "\n",
    "row_count = (len(df))\n",
    "run.log('observaciones totales:', row_count)\n",
    "\n",
    "embarazos = df['Pregnancies'].value_counts().reset_index().sort_values(by='index')\n",
    "embarazos.set_index('index',inplace=True)\n",
    "print(embarazos.head())\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "ax = fig.gca()    \n",
    "embarazos.plot.bar(ax = ax) \n",
    "ax.set_title('Distribución') \n",
    "ax.set_xlabel('Embarazos') \n",
    "ax.set_ylabel('Pacientes')\n",
    "plt.show()\n",
    "run.log_image(name = 'distribución embarazos', plot = fig)\n",
    "\n",
    "edades = df.Age.unique()\n",
    "run.log_list('edades dataset', edades)\n",
    "\n",
    "descripcion = df.describe().reset_index()\n",
    "import numpy as np\n",
    "np.array(descripcion)\n",
    "run.log_table('descripción dataset',{x:list(descripcion[x].values) for x in descripcion.columns})\n",
    "\n",
    "run.complete()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nos acercamos al momento de la verdad\n",
    "Ya que tenemos nuestros datos correctamente cargados con su versionamiento respectivo, entendimos como funcionaban los experimentos y los logs, podemos avanzar a hacia la creación de modelos.\n",
    "## ¡Ambientes!\n",
    "Docker hace bastante tiempo es una gran alternativa ante la dificultad que implica generar ambientes replicables. Azure nos entrega la capacidad de usarlo, sin tener idea de como armar un Dockerfile. El ambiente se separa por Python y Docker, y cada uno de ellos tiene sus propias adecuaciones (como las dependencias de conda).\n",
    "## Registrar el ambiente\n",
    "Al igual que en los casos anteriores, podemos registrar el ambiente en el workspace. Esto nos entrega todos los beneficios de replicabilidad asociados al versionamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "diabetes_env = Environment(\"diabetes-experiment-env\")\n",
    "diabetes_env.python.user_managed_dependencies = False\n",
    "diabetes_env.docker.enabled = True\n",
    "\n",
    "diabetes_packages = CondaDependencies.create(conda_packages=['scikit-learn==0.22.2.post1'],\n",
    "                                          pip_packages=['azureml-defaults'])\n",
    "\n",
    "diabetes_env.python.conda_dependencies = diabetes_packages\n",
    "\n",
    "diabetes_env.environment_variables = {\"MESSAGE\":\"Hello from Azure Machine Learning\"}\n",
    "\n",
    "print(diabetes_env.name, 'defined.')\n",
    "\n",
    "diabetes_env.register(workspace=ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creamos la imagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Image\n",
    "build = diabetes_env.build(workspace=ws)\n",
    "build.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¡Creamos scripts reutilizables!\n",
    "Una gran característica que ofrece Azure Machine Learning, es que podemos entregar scripts como uno de los parámetros de ejecución para un Estimator. ¿Qué es un Estimator? Bueno al parecer me estoy adelantando, vamos a crear la carpeta primero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "experiment_folder = 'diabetes_training_from_tab_dataset'\n",
    "os.makedirs(experiment_folder, exist_ok=True)\n",
    "print(experiment_folder, 'folder created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definiendo el script donde entrenaremos nuestro modelo\n",
    "En la siguiente celda, importamos todos los paquetes necesarios y utilizamos argparse para poder incluir parámetros entregados desde afuera a nuestro código. Es importante destacar también que para inicializar el \"run\" es necesario utilizar get_context(), para contextualizar el experimento que se entregó desde afuera.\n",
    "## Entrenamiento del modelo\n",
    "Como vamos a entender más adelante, debemos entregar como input nuestros datos. Además  podemos registrar las métricas que más podrían interesarnos. En este caso evaluaremos el accuracy y el área bajo la curva ROC. El modelo a entrenar será un Random Forest e intentaremos predecir si se diagnóstica o no diabetes. Como último paso debemos guardar el modelo en la carpeta indicada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $experiment_folder/diabetes_training.py\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "from azureml.core import Run\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--information', type=str, dest='criterion', default='gini', help='entropy or gini')\n",
    "parser.add_argument('--depth', type=float, dest='max_depth', default=7, help='max depth')\n",
    "args = parser.parse_args()\n",
    "criterion = args.criterion\n",
    "max_depth = args.max_depth\n",
    "\n",
    "run = Run.get_context()\n",
    "\n",
    "print(\"Loading Data...\")\n",
    "diabetes = run.input_datasets['diabetes'].to_pandas_dataframe()\n",
    "\n",
    "X, y = diabetes[['Pregnancies','PlasmaGlucose','DiastolicBloodPressure','TricepsThickness','SerumInsulin','BMI','DiabetesPedigree','Age']].values, diabetes['Diabetic'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=40, stratify = y)\n",
    "\n",
    "\n",
    "print('Training a Random Forest Regressor', criterion)\n",
    "run.log('information',  np.array(criterion))\n",
    "model = RandomForestClassifier(criterion=criterion,max_depth=max_depth,random_state=40).fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_hat = model.predict(X_test)\n",
    "acc = np.average(y_hat == y_test)\n",
    "print('Accuracy:', acc)\n",
    "run.log('Accuracy', np.float(acc))\n",
    "\n",
    "y_scores = model.predict_proba(X_test)\n",
    "auc = roc_auc_score(y_test,y_scores[:,1])\n",
    "print('AUC: ' + str(auc))\n",
    "run.log('AUC', np.float(auc))\n",
    "\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "joblib.dump(value=model, filename='outputs/diabetes_model.pkl')\n",
    "\n",
    "run.complete()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimadores\n",
    "Para conseguir una ejecución en nuestro ambiente, debemos específicar otros diferentes requerimientos. Entre ellos los inputs, el script, el compute_target y el ambiente. Todas estas funcionalidades y más se pueden conseguir con los Estimator de Azure Machine Learning. \n",
    "## Parámetros\n",
    "Como vemos en la siguiente celda, tenemos que entregar como parámetros todos los requirimientos de los que hablamos. Por ejemplo para llamar al ambiente, podemos invocarlo por su nombre y nos entregará por default la última versión.\n",
    "## Seguimiento de la ejecución\n",
    "Para ir revisando los logs de la ejecución utilizamos RunDetails(), de esta forma sabemos en que parte de la construcción del ambiente estamos, como también podemos ir viendo directamente las métricas que nos interesaba recabar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import Model\n",
    "from azureml.train.estimator import Estimator\n",
    "\n",
    "from azureml.core import Experiment\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "for i in [['gini',13],['entropy',7]]:\n",
    "    # Set the script parameters\n",
    "    script_params = {\n",
    "    '--information': i[0],\n",
    "    '--depth': i[1]\n",
    "    }\n",
    "\n",
    "# Get the training dataset\n",
    "    diabetes_ds = Dataset.get_by_name(workspace = ws, name='diabetes-azure', version=2)\n",
    "\n",
    "# Create an estimator\n",
    "    estimator = Estimator(source_directory=experiment_folder,\n",
    "                      inputs=[diabetes_ds.as_named_input('diabetes')],\n",
    "                      entry_script='diabetes_training.py',\n",
    "                      compute_target='local',\n",
    "                      environment_definition=Environment.get(workspace=ws,name='diabetes-experiment-env')\n",
    "                      )\n",
    "\n",
    "# Create an experiment\n",
    "    experiment_name = 'diabetes-training-1'\n",
    "    experiment = Experiment(workspace = ws, name = experiment_name)\n",
    "\n",
    "# Run the experiment\n",
    "    run = experiment.submit(config=estimator)\n",
    "# Show the run details while running\n",
    "    RunDetails(run).show()\n",
    "    run.wait_for_completion()\n",
    "\n",
    "run.register_model(model_path='outputs/diabetes_model.pkl', model_name='diabetes_model',\n",
    "                   tags={'Training context':'Inline Training'},\n",
    "                   properties={'AUC': run.get_metrics()['AUC'], 'Accuracy': run.get_metrics()['Accuracy']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuración y registro del modelo\n",
    "Como vemos, necesitamos distintos parámetros para poder registrar el modelo, los parámetros, model_framework y resource_configuration, nos van a permitir hacer el deployment a una ACI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.resource_configuration import ResourceConfiguration\n",
    "\n",
    "model_path=Model.get_model_path('diabetes_model',_workspace=ws)\n",
    "model = Model.register(workspace=ws, model_name='diabetes_model',\n",
    "                       model_path=model_path,\n",
    "                   tags={'Training context':'Inline Training'},\n",
    "                   properties={'AUC': run.get_metrics()['AUC'], 'Accuracy': run.get_metrics()['Accuracy']},\n",
    "                  model_framework=Model.Framework.SCIKITLEARN,\n",
    "                  model_framework_version='0.22.2.post1',\n",
    "                   resource_configuration=ResourceConfiguration(cpu=1, memory_in_gb=0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script de inferencia\n",
    "Cuando queremos customizar nuestros despliegues, ya sea para usar nuestros propias imagenes o para poner en producción modelos en AKS, necesitamos configurar scripts de inferencia. Estos scripts son bastante standard y consisten de dos métodos init y run. Basta cargar el modelo correctamente en init y generar la predicción en run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile score.py\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import joblib\n",
    "def init():\n",
    "    global model\n",
    "    model_path = Model.get_model_path('diabetes_model')\n",
    "    model = joblib.load(model_path)\n",
    "def run(data):\n",
    "    try:\n",
    "        result = model.predict(data)\n",
    "        return result.tolist()\n",
    "    except Exception as e:\n",
    "        error = str(e)\n",
    "        return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment!\n",
    "Finalmente estamos llegando al despliegue de nuestro modelo. En este caso el deployment será a una ACI, que es el caso propuesto para testeo. AKS o Azure Ml Compute son mejores opciones para producción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import Model\n",
    "import joblib\n",
    "\n",
    "from azureml.core.model import InferenceConfig\n",
    "inference_config = InferenceConfig(entry_script=\"score.py\", \n",
    "                                   environment=diabetes_env)\n",
    "\n",
    "\n",
    "service_name = 'my-diabetes-service-1'\n",
    "service = Model.deploy(ws, service_name, [model])\n",
    "\n",
    "service.wait_for_deployment(True)\n",
    "print(service.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vamos a aprovechar nuestro modelo entonces\n",
    "Esta es una de las distintas formas en la que podemos invocar los resultados del modelo. Pero también hay opciones para llamarlo directamente de aplicaciones. Para no salirnos de nuestro Jupyter notebook, les voy a mostrar a continuación la forma para hacerlo desde este ambiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "input_payload = json.dumps({\n",
    "    'data': [\n",
    "        [ 2,  170,  80, 34, 200,\n",
    "         40, 1.1, 50]\n",
    "    ],\n",
    "    'method': 'predict_proba' \n",
    "})\n",
    "\n",
    "output = service.run(input_payload)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probemos solo con predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "input_payload = json.dumps({\n",
    "    'data': [\n",
    "        [ 2,  170,  80, 34, 200,\n",
    "         40, 1.1, 50]\n",
    "    ],\n",
    "    'method': 'predict'  \n",
    "})\n",
    "\n",
    "output = service.run(input_payload)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terminamos!!\n",
    "Espero que todo lo que aprendimos te haya sido de utilidad. Aprendimos sobre ambientes, datos, modelos y ambientes productivos. Fueron unas dos horas largas, pero te aseguro que te va a resultar útil adquirir esta nueva skill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
